# Experiment
一些小练习


树的初始化

一种为递归地初始化，在构造函数内递归调用构造函数，设置好退出递归的条件即可

另一种为DFS初始化，在遍历树的过程中初始化节点，设置好退出循环的条件


实现机器学习方法的步骤：

得到一个有限的训练数据集合

确定包含所有可能的模型的假设空间，即学习模型的集合

确定模型选择的准则，即学习的策略

实现求解最优模型的算法，即学习的算法

通过学习方法选择最优模型

利用学习的最优模型对新数据进行预测或分析

线性回归，连续变量预测，输出为实数，用最小二乘法来计算参数，容易受到异常值的影响

试图学得f(x_i)=wx_i+b，使得f(x_i) \approx y_i

最小二乘法估计基于均方差最小化的方法估计参数w与b

求解w与b使得\sum_{i=1}^N(y_i-wx_i-b)^2最小，分别对两个参数求导，令结果为0，得到：

w=\frac{\sum_{i=1}^{N}y_i(x_i-\overline{x})^2}{\sum_{i=1}^{N}x_i^2-\frac{1}{n}(\sum_{i=1}^{N}x_i)^2},b=\frac{1}{n}\sum_{i=1}^{N}(y_i-wx_i)

推广到多元线性回归，求解W即可

y=W^TX,W=(w_0,w_1,w_2,\cdots)^T,X=(1,x_1,x_2,\cdots)^T

logistic回归，用于分类，输出为0到1之间的概率值，用最大似然估计来计算参数，对异常值有较好的稳定性

logistic回归假设数据服从伯努利分布，因此LR属于参数模型，扩展w与x，将b作为w的最后一个分量，1作为x的最后一个分量：

P(Y=1|x)=\frac{exp(wx)}{1+exp(wx)},P(Y=0|x)=\frac{1}{1+exp(wx)}

似然函数

\prod_{i=1}^{N}{P(Y=1|x_i)^{y_i}P(Y=0|x_i)^{1-y_i}}

对数似然函数

\sum_{i=1}^{N}[{y_i(w\cdot x_i)-\log (1+exp(w\cdot x_i))]}

再用梯度下降法或拟牛顿法估计参数w即可得到logistic回归模型，再用模型进行分类，亦可推广到多分类的情况
